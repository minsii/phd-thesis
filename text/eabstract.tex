Since multicore processor chips are the norm today, the only way to 
improve the performance for high end processors is to add more threads 
and cores. Many core architecture, such as Intel Xeon Phi and Blue Gene/Q, 
provides us such a massively parallel environment with dozens of cores and
hundreds of hardware threads. To efficiently utilize such architectures, 
application programmers are increasingly looking at hybrid programming 
models comprising a mixture of processes and threads (frequently called 
``MPI+X'' models). A common mode of operation for such applications 
uses multiple threads to parallelize the computation, while one of the 
threads also issues MPI operations (i.e., MPI \texttt{FUNNELED} or 
\texttt{SERIALIZED} thread-safety mode). Although such model extremely 
improves floating point performance for most applications, still 
some of them lose performance at the bottleneck of MPI communication.

This thesis focuses on exploiting the capabilities of many core 
architectures on widely used MPI implementation, in order to eliminate
the communication bottleneck and consequently achieve highly efficient 
performance for user applications. It is broadly divided into two related 
aspects of utilizing the features of such architecture: \textit{parallelism}
and \textit{asynchronous progress}.

% parallelism
Firstly, we deeply studied the capabilities of \textit{parallelism} inside 
communication associated with various MPI models. 
Most applications implement hybrid MPI+threads model by 
utilizing OpenMP, the most prominent of the threading models used 
in scientific computing today, to parallelize their computation.
In MPI+OpenMP applications, the common \texttt{FUNNELED}\slash \texttt{SERIALIZED} 
mode is achieved, for example, by placing MPI calls in OpenMP critical 
sections or outside the OpenMP parallel regions. However, such a model 
often means that the OpenMP threads are active only during the parallel 
computation phase and idle during the MPI calls, resulting in wasted 
computational resources. Moreover, the sequential communication phase, 
which is executed by single light-weight core, may even degrade 
performance. In this thesis, we first present MT-MPI, an internally 
multithreaded MPI implementation that transparently coordinates
with the threading runtime system to \textit{share idle threads} with 
the application. It is designed in the context of OpenMP and
requires modifications to both the MPI implementation and the OpenMP
runtime in order to share appropriate information between them. We
demonstrate the benefit of such internal parallelism for various
aspects of MPI processing, including derived datatype communication,
shared-memory communication, and network I/O operations.

Despite the increasing trends in hybrid MPI+threads applications,
a large number of MPI applications still function in an MPI-only 
mode (MPI \texttt{THREAD\_SINGLE} thread-safety mode), where an MPI 
process is launched for each available core on the system. Such a model, 
however, not only suffers from the partitioning of resources across 
processes, but also from loss of performance in a number of ways. For 
example, if an MPI process is waiting for a message to arrive, the 
core on which it is scheduled is idle and underutilized. In this 
thesis, we plan to utilize the concept of \textit{user-level processes}, 
a way to provide multiple co-scheduled ``OS processes'' on a single core, 
so that a large number of MPI processes are able to be executed on a 
single core and achieve better load balancing and light-weight 
checkpoint migration.

% asynchronous progress
Secondly, we focus on \textit{asynchronous progress} for improving 
the communication and computation overlap in order to minimize 
the communication bottleneck. 
The MPI-2 standard introduced one-sided communication semantics 
(also known as RMA) that allow one process to specify all communication 
parameters, for both the sending side and the receiving side. Thus, a 
process can access memory regions of other processes in the system 
without the target process explicitly needing to receive or process 
the message.RMA has the potential to deliver better communication and 
computation overlap than traditional two-sided communication does, 
particularly on networks that support one-sided communication natively, 
such as InfiniBand or Cray Aries interconnects. However, the MPI standard 
does not guarantee that such communication is truly asynchronous. Thus, 
most MPI implementations require the remote target to make MPI calls 
to ensure progress on such operations. To work around this issue, we 
present Manticore, a process-based asynchronous progress model 
for MPI on many-core architectures. Manticore uses a combination 
of MPI-3 shared-memory windows and PMPI-based redirection of RMA 
operations to assist RMA operations that require software intervention 
for progress without affecting hardware-based RMA operations.  We 
describe the design of Manticore and compare it with other approaches 
for asynchronous progress. Preliminary evaluation results show that 
Manticore offers improved performance and good scalability.

Although shared window memory helps truly asynchronous one-sided 
communication, a more generalized design is still necessary for all the 
communication modes. We plan to extend Manticore utilizing special memory 
mapping technology in order to fully handle asynchronous progress for 
both one-sided and two-sided communication.

\clearpage

