%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{MPI One-sided Communication}\label{sec:back-rma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

MPI-2 and MPI-3 introduced the MPI one-sided communication model
(also known as remote memory access or RMA). Unlike the well-known
two-sided communication (e.g., MPI\_Send\slash MPI\_Receive), the one-sided
mode allows applications to define more dynamic and data-driven communication
patterns where a process can directly access memory in another process
(i.e., window) through RMA operations such as \emp{put}, \emp{get} or
\emp{update}. Furthermore, all the operations are only issued from the
origin process, thus the program running on the remote process does not
need to call any MPI routines to match the operations. Figure~\ref{fig:back-rma}
demonstrates the difference between the two-sided mode and the one-sided mode.

\begin{figure}%[ht]
\centering
\subfigure[Two-Sided Mode.] {
  \includegraphics[height=0.28\textwidth]{figures/background/mode-rma-two-sided.pdf}
  \label{fig:back-rma-two-sided}
}
\hspace{-0.02\textwidth}
\subfigure[One-Sided Mode.] {
  \includegraphics[height=0.28\textwidth]{figures/background/mode-rma-one-sided.pdf}
  \label{fig:back-rma-one-sided}
}
\caption{MPI Communication Modes}
\label{fig:back-rma}
\end{figure}



%===============================
\subsection{Programming and Semantics}
%===============================

Because the second contribution of this thesis is a process-based asynchronous
progress model that comprehensively supports the strict MPI one-sided
communication semantics which is also the most challenging part in this work,
we then introduce the primary semantics of this communication mode in the rest
of this section. The semantics of the RMA communication can be divided into
following three primary steps: window creation, RMA synchronization and
issuing RMA operations.


%--------------------------
\subsubsection{Window Creation}
%--------------------------

A memory area on a process that is exposed to all other processes in a
specified group---allowing direct access to these processes---is called
a \emp{ window}. MPI-3 provides the following four window initialization
functions:

\begin{description}
\item[MPI\_Win\_create]\hfill \\
This routine exposes an RMA window for the memory region which is allocated
by user application in advance. The corresponding \fn{MPI\_Win\_free}
call only releases the RMA window, thus user is responsible for releasing
the memory region after window is freed.

\item[MPI\_Win\_allocate]\hfill \\
This routine allows MPI to internally allocate a memory region and expose
it to the other processes as a remotely accessible window. The corresponding
\fn{MPI\_Win\_free} call releases both the window structure and the memory
buffer.

\item[MPI\_Win\_allocate\_shared]\hfill \\
This routine allows MPI to initialize a shared window among processes
located in the same shared memory system (e.g., the same NUMA node) through
external system support such as mmap or XPMEM on Cray systems~\cite{xpmem}.
This shared memory region can be accessed by CPU load\slash store instructions
instead of MPI RMA operations, however, additional synchronization is required
to ensure the correctness with other concurrent RMA operations. The start
address of a remote window region mapped on the local process can be got from
the \fn{MPI\_Win\_shared\_query} call.

\item[MPI\_Win\_create\_dynamic]\hfill \\
This routine allows programs to expose an empty remote accessible window,
and then attach\slash detach one or multiple memory regions in later execution.
\end{description}

The above routines give user different levels of flexibility of window
creation. However, the routines with more flexibility also limit the possible
internal optimization can be provided from MPI implementations. For instance,
the most flexible \fn{MPI\_Win\_create\_dynamic} can rarely get any
optimization.

%------------------------------
\subsubsection{RMA Operations}
%------------------------------

After the remote accessible window is identified, a process can issue
\emp{put}, \emp{get}, or \emp{accumulate} operations to access this
window. Figure~\ref{fig:back-rma-one-sided} gives an image to demonstrate
data movement associated to those operations.

\begin{description}
\item[MPI\_Put]\hfill \\
This operation copies the data in the origin process's buffer to
the specified memory location in the window on the target process.

\item[MPI\_Get]\hfill \\
This operation copies data from the specified memory location
of remote window to the buffer located in the origin process's
local memory.

\item[MPI\_Accumulate]\hfill \\
This operation first transfers data from the origin process's buffer
to the target process, and then performs a update on the target side
following the user specified operation (e.g., \fn{MPI\_SUM}) and stores
the result into the window. We note that, unlike the put\slash get
operations, the accumulate operation is guaranteed to be \emp{ordered} and
\emp{atomic} per basic data element.
% We will introduce the ordering and
% atomic semantics in Section~\ref{sec:back-rma-semantic}.
\end{description}

Beside above three basic RMA operations, there are three other
operations also defined in MPI standard: \fn{MPI\_Get\_accumulate},
\fn{MPI\_Fetch\_and\_op} and \fn{MPI}-\fn{\_Compare\_and\_swap}. The detailed
semantics of those operation can be found in~\cite{mpi30-report}.

%------------------------------
\subsubsection{RMA Synchronization}
%------------------------------

All the RMA operations are non-blocking MPI calls, which means the completion
of those data movement is not guaranteed at return. In addition, since processes
may concurrently access the same RMA window, we also need synchronization
among the involved processes in order to avoid any conflicts. MPI defines two
kinds of synchronization modes to handle those responsibilities in RMA communication,
they are the \emp{active mode} and the \emp{passive mode}. We introduce
each of them separately in this section.

\begin{figure}%[ht]
\centering
\subfigure[Fence.] {
  \includegraphics[height=0.25\textwidth]{figures/background/mode-rma-fence.pdf}
  \label{fig:back-rma-fence}
}
% \hspace{0.01\textwidth}
\subfigure[Post-Start-Complete-Wait.] {
  \includegraphics[height=0.25\textwidth]{figures/background/mode-rma-pscw.pdf}
  \label{fig:back-rma-pscw}
}
\\
\subfigure[Lock\_all-Unlock\_all.] {
  \includegraphics[height=0.26\textwidth]{figures/background/mode-rma-lockall.pdf}
  \label{fig:back-rma-lockall}
}
% \hspace{-0.06\textwidth}
\subfigure[Lock-Unlock.] {
  \includegraphics[height=0.26\textwidth]{figures/background/mode-rma-lock.pdf}
  \label{fig:back-rma-lock}
}
\caption{RMA Synchronization Modes}
\label{fig:back-rma-sync}
\end{figure}

\parahead{Active Mode}:
This mode provides a similar synchronization as that in two-sided mode,
both the origin process and the target process need to explicitly
call the synchronization. Two sets of synchronization calls are defined
in MPI: \emp{fence} and \emp{post-start-complete-wait}.

\begin{itemize}
  \item In \emp{fence} synchronization, all the processes in the window
  must collectively call \fn{MPI\_Win\_Fence} routine to synchronize with
  each other (Figure~\ref{fig:back-rma-fence}) similar as \emp{barrier} in the
  two-sided communication mode. The return from the fence call guarantees:
  (1) all the processes have arrived at the fence call; (2) all the outstanding
  RMA operations and local load\slash store instructions issued on this window
  have been completed.

  \item The \emp{post-start-complete-wait} synchronization can be considered as
  a subset of fence (Figure~\ref{fig:back-rma-pscw}). At the beginning of
  the RMA communication, the target process (P1) calls \fn{MPI\_Win\_post}
  to expose its window to one or several processes and the origin process
  (P0 or P2) performs \fn{MPI\_Win\_start} to match the post call and then
  starts the remote access; at the end of the communication, the origin
  process needs to call \fn{MPI\_Win\_complete} to complete its operations
  and the target process needs to call \fn{MPI\_Win\_wait} to ensure all
  the operations issued on its window have been finished.
\end{itemize}

\parahead{Passive Mode}:
Apart from the semi-dynamic active mode, MPI also offers the passive mode
which performs completely dynamic pattern. That is, only the process
issuing operations (origin process) is required to explicitly call the
synchronization. Two sets of synchronization calls are defined:
\emp{lock\_all-unlock\_all} and \emp{lock-unlock}.

\begin{itemize}
  \item The \emp{lock\_all} serial provides global synchronization similar
  as the \emp{fence}, however, only the origin process (e.g., P0 in
  Figure~\ref{fig:back-rma-lockall}) issues the \fn{MPI\_Win}-\fn{\_lock\_all}
  and \fn{MPI\_Win\_unlock\_all} calls. The return from \emp{lock\_all}
  ensures the origin process have acquired the \emp{shared} lock on all
  the other processes, and the return from \emp{unlock\_all} ensures:
  (1) the locks have been released, and (2) all the operations issued from
  this process have been completed remotely.

  \item The \emp{lock} serial can be also considered as a subset of \emp{lock\_all}
  which provides per-target exclusive\slash shared lock (\fn{MPI\_LOCK\_EXCLUSIVE}
  or \fn{MPI\_LOCK\_SH}-\fn{ARED} lock type). We note that two origin processes can
  concurrently acquire a \emp{shared} lock on the same target window, however,
  any other lock requests must be serialized with the \emp{exclusive} lock.
  Figure~\ref{fig:back-rma-lock} shows an example. Simultaneous \emp{lock\_all}
  and \emp{lock} follow the same rule.

  \item Besides the lock calls, the passive mode also offers two \emp{flush}
  synchronization routines that only complete the outstanding RMA operations,
  and a \emp{sync} routine (\fn{MPI\_Win\_sync}) that synchronizes the data
  of its local window updated by remote RMA operations and the one updated
  by load\slash store instructions. \fn{MPI\_Win\_flush} completes the operations
  issued from the origin process to a single target process, and \fn{MPI\_Win\_flush\_all}
  completes operations issued from the origin process to any target processes
  in the window.
\end{itemize}

% \end{description}

% %------------------------------
% \subsection{Ordering and Atomicity}\label{sec:back-rma-semantic}
% %------------------------------

% In the section, we introduce the ordering and atomicity semantics
% guaranteed for \emp{accumulate}-like operations (i.e., \emp{accumulate},
% \emp{get\_accumulate}, \emp{fetch\_and\_op}, \emp{compare\_and\_swap})
% in MPI standard by using two examples for each.

% Any MPI implementation must guarantee the ordering between accumulate-like
% operations that are issued from the same origin process and sent to the
% same target. Figure~\ref{fig:code_rma_ordering} gives an example.

% \newsavebox\mpiRmaOrdering
% \begin{lrbox}{\mpiRmaOrdering}
% \begin{lstlisting}[linewidth=0.7\columnwidth]
% P0                              P1
%                                 x = 2;
% Lock(P1, win);
% Get_accumualte(x, y, +1, P1);
% Accumualte(x, +2, P1);
% Unlock(P1, win);
% Read y;
% \end{lstlisting}
% \end{lrbox}

% \begin{figure}[ht]
% \centering
% \vspace{2.0ex}
% \usebox\mpiRmaOrdering
% \caption{Example of Ordering Semantics in MPI RMA communication.}
% \label{fig:code_rma_ordering}
% \end{figure}

% MPI also guarantees the atomicity of any accumulate-like operations
% that are issued to the same target process and the same window memory
% location per basic data element (e.g., integer or double, but any
% MPI derived datatype is not included). The operations can be issued
% from the same origin process or from different ones.
% Figure~\ref{fig:code_rma_atomic} gives an example.


% \newsavebox\mpiRmaAtomic
% \begin{lrbox}{\mpiRmaAtomic}
% \begin{lstlisting}[linewidth=1\columnwidth]
% P0                      P1                      P2
%                                                 x = 2;
% Lock(SHARED, P2, win);  Lock(SHARED, P2, win);
% Accumualte(x, +1, P1);
%                         Accumualte(x, +2, P1);
% Unlock(P2, win);        Unlock(P2, win);
% \end{lstlisting}
% \end{lrbox}

% \begin{figure}[ht]
% \centering
% \vspace{2.0ex}
% \usebox\mpiRmaAtomic
% \caption{Example of Atomicity Semantics in MPI RMA communication.}
% \label{fig:code_rma_atomic}
% \end{figure}


%===============================
\subsection{Irregular Applications}
%===============================
In this section, we introduce three scientific applications in chemistry,
bioinformatics and nuclear physics fields, all of them involve extremely
dynamic and irregular computation and data movement, which can benefit
from the one-side communication model.
% To fully utilize advanced HPC
% systems, computational scientists are looking at the ways to improve
% these applications in order to solve larger problem sizes with highly
% accurate results.

%------------------------------
\subsubsection{NWChem Quantum Chemistry Application}
%------------------------------
NWChem is a widely used quantum chemistry application suite that provides
a large set of simulation capabilities~\cite{nwchem}. Due to the large
memory needs in NWChem that often require memory sharing across multiple
nodes, it is developed based on the Global Arrays toolkit~\cite{GA_SC94}
which provides users with distributed dense arrays that can be accessed
through one-sided operations. Figure~\ref{fig:app-nwchem-pattern}
demonstrates the typical \emp{get-compute-update} pattern used in NWChem.

Current NWChem has been looking at small-to-medium molecules (e.g.,
(H$_2$O)$_{21}$ as shown in Figure~\ref{fig:app-nwchem-w21}) consisting of
20-100 atoms. Since the coulomb interaction among such small amount of
atoms is reasonably large, the computation and communication can be
successful scaled on modern supercomputers. However, scientists aim to
study more complex molecules that are composed of thousands atoms or even
larger thus not only the short-range interactions but also the long-range
interactions have to be covered. This means, the diversity of the amount
of computation per process can be considerably increased, thus resulting
in extremely irregular computation with data movement.

\begin{figure}[ht]
\centering
\subfigure[Interaction in (H$_2$O)$_{21}$ molecule.] {
\includegraphics[width=0.38\textwidth]{figures/background/app-nwchem-w21.pdf}
\label{fig:app-nwchem-w21}}
\subfigure[Get-Compute-Update pattern.] {
\includegraphics[width=0.56\textwidth]{figures/background/app-nwchem-get-comp-update.pdf}
\label{fig:app-nwchem-pattern}}
\caption{Communication in NWChem Application.}
\label{fig:app-nwchem}
% \vspace{-2.0ex}
\end{figure}


\subsubsection{SWAP-Assembly Bionformatics Application}

In bioinformatics, since it is still hard to read the whole genomes
in modern DNA sequencing technology, researchers often break down long
DNA samples into large amount of small fragments, called ``reads'', and
then read those reads into digital data as the first step. The next step
is called assembly, which merges overlapping reads back to one or several
contiguous DNA sequences. The sequence assembly technology helps biology
scientists analyze DNA sequence, and is especially important for understanding
complex environments containing many different microbiomes (e.g., soil and
seawater).

The SWAP-Assembler software provides highly scalable assembler that processes
the sequence assembly on thousands of cores in parallel~\cite{swap}. The
initial reads are represented as a distributed De Bruijn graph (e.g., )
Figure~\ref{fig:app-swap-graph}), and final contiguous DNA chains are
assembled by executing multiple rounds of graph reduction and error removal
over MPI communication. The communication pattern follows the
\emp{send-merge-return} mode as shown in Figure~\ref{fig:app-swap-merge}.
Every process issues each of its local DNA read to a remote process to find
the overlapping reads. On the remote process, it first searches the
overlapping read for every received message, then merges the reads and
finally returns the merged result. If there is no matching read on the
remote side, the sending process will try another remote process following
the same patter. This processing always involves enormous irregular data
movement over petabytes of data and requires several days or even months of
computation. For instance, the largest simulation done to date was at the
University of Chicago, where a 2.3-terabyte sample was assembled on a
supercomputer with 18,000 cores—this simulation took 4 days to complete and
spent 99.9\% of its time idling, because of imbalance between processing
units.


\begin{figure}[ht]
\centering
\subfigure[Graph Reduction.] {
\includegraphics[width=0.3\textwidth]{figures/background/app-swap-graph.pdf}
\label{fig:app-swap-graph}}
\subfigure[Communication Pattern.] {
\includegraphics[width=0.65\textwidth]{figures/background/app-swap-merge.pdf}
\label{fig:app-swap-merge}}
% \includegraphics[width=0.9\columnwidth]{figures/background/swap-merge.pdf}
\caption{Irregular Communication in SWAP-Assembly.}
\label{fig:app-swap}
% \vspace{-2.0ex}
\end{figure}

% \subsection{Nuclear Reactor Core Modeling}
% \subsection{Graph-Type Parallel Visualization}

%------------------------------
\subsubsection{Green’s Function Monte Carlo}
%------------------------------
Green’s Function Monte Carlo (GFMC) is an application in theoretical nuclear
physics that provides ab initio calculations for few-nucleon systems~\cite{gfmc}.
It describes the nuclear structures and reactions by solving the
Schr\"{o}dinger equation and is recognized as the most reliable method
for nuclei with 12 or fewer nucleons. The implementation of GFMC utilizes
OpenMP to parallelize heavy sparse matrix-vector multiplications and
uses MPI to communicate among distributed computing nodes.

The Asynchronous Dynamic Load Balancing (ADLB) library~\cite{adlb} is
essentially designed for addressing the load balancing among MPI processes
in GFMC on large scale systems that contain more than one hundred thousand
computing cores. It provides a general-purpose worker-server model with
one-sided Put/Get operations that helps application codes dynamical
share work tasks with assorted work types and priorities. A few server
processes are initialized to maintain a distributed shared work queue.
Application processes can then submit arbitrary work tasks to the queue
with necessary data, and retrieve the results after any task is finished.
